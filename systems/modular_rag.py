from components.document_loader import load_documents
from components.chunker import chunk_documents
from components.embedder import generate_embeddings
from components.retriever import Retriever
from components.reranker import rerank_with_qwen_ollama
from components.prompt_builder import build_prompt
from langchain_ollama import OllamaLLM


def run_modular_rag(file_path: str, query: str, use_reranker: bool = True):
    """
    Fully modular RAG pipeline:
    - Manual control over loading, chunking, embedding, retrieval, reranking, and prompt building.

    Args:
        file_path (str): Path to input document.
        query (str): User question.
        use_reranker (bool): Whether to rerank retrieved chunks before sending to LLM.

    Returns:
        str: Final answer generated by LLM.
    """
    # Load and chunk
    filename = file_path.split("\\")[-1]
    documents = load_documents(filename, file_path)
    chunks = chunk_documents(documents)
    chunk_texts = [doc.page_content for doc in chunks]

    # Embed chunks
    embeddings = generate_embeddings(chunk_texts)

    # Initialize FAISS retriever
    dimension = len(embeddings[0])
    retriever = Retriever(embeddings=embeddings, chunks=chunk_texts, dimension=dimension)

    # Embed query and retrieve top-k
    query_embedding = generate_embeddings([query])[0]
    top_chunks = retriever.search(query_embedding, top_k=5)
    top_texts = [chunk for chunk, _ in top_chunks]

    # Optional reranking
    if use_reranker:
        top_texts = rerank_with_qwen_ollama(query, top_texts, top_k=3)

    # Build prompt
    prompt = build_prompt(query, top_texts)

    # LLM response
    llm = OllamaLLM(model="llama3.2")
    response = llm.invoke(prompt)

    return response
